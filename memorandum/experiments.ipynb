{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "結局Embeddingのサイズの小ささからMiniLMを使ったけど、SimCSE系の方が文の意味の要約としては良さそう (が、ベクトルの次元数がでかい…)"
      ],
      "metadata": {
        "id": "PW34K9HqWCwY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9O6ih6wFWjt",
        "outputId": "91f652b3-83ee-4c0e-d2dc-cae093eeddb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Score (Original): 0.6349182\n",
            "Cosine Similarity Score (4-bit Quantized): 0.9533956402920651\n",
            "Encoded Fingerprint: zwt3ifcNrR5MFCgNU3vaVsmdrjZcbQivoPvs9TyGeoVAp1E7i6bFyKnXD6xjrS3cEHmQnPhZtxFbaiRi0Vm3ZnXk9RYsmlB1ho2ftirkeif6Wg1PKedfCPZdgOYcNBFS6DGceKapWK4yzw9hWlsz3ilYCsTNAZg7CeiguLvvfILnEA4j6Uth2Kzn5Jf7cYEMC4OpBas1fmvqYu0B4tnTpixRVGGNYMcVARvR70xnDZ2knGOIfOcdZes1f4RL6lXT6L\n",
            "Encoded Fingerprint2: mwak1XymR0bWCsBj7IVP9jC8w8RZaGlNvNSziT1Iv2zLchQnIgFsLdGTH0wK8KeiXwV38rwOXxyIywA96qNd2H01X2UOtF1d51nKbT48qduoJ0jaRkxILCQKP1NHq3h76QbYwF5s60BxlaFcO9pdGr9HaN1iSWGHh6HJa2RBOaMvEw4jlWFtiwldnGMPZ6iwqAIpgZNawWn7Bb3fKcHHRfK9PLH3spXnQD6McdYMFwGdfAKxxOQAmqJdmjW4hHdCYO\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import string\n",
        "\n",
        "# Initialize the SBERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def generate_semantic_fingerprint(document):\n",
        "    # Encode the document and return a 1D vector (embedding)\n",
        "    return model.encode([document])[0]\n",
        "\n",
        "def quantize_to_n_bits(vector, n_bits):\n",
        "    # Calculate the min and max of the vector for scaling\n",
        "    min_val, max_val = np.min(vector), np.max(vector)\n",
        "    # Calculate the number of bins for n-bit quantization (2^n)\n",
        "    n_bins = 2 ** n_bits\n",
        "    # Define bin edges\n",
        "    bins = np.linspace(min_val, max_val, n_bins)\n",
        "    # Quantize the vector to integer indices (bin assignments)\n",
        "    quantized_vector = np.digitize(vector, bins) - 1  # digitize starts from 1\n",
        "    # Clip values to ensure they're within [0, n_bins-1]\n",
        "    quantized_vector = np.clip(quantized_vector, 0, n_bins - 1)\n",
        "    return quantized_vector\n",
        "\n",
        "def base62_encode(num):\n",
        "    # Characters for base62 encoding\n",
        "    characters = string.digits + string.ascii_letters\n",
        "    base = 62\n",
        "    if num == 0:\n",
        "        return characters[0]\n",
        "    encoding = ''\n",
        "    while num > 0:\n",
        "        num, rem = divmod(num, base)\n",
        "        encoding = characters[rem] + encoding\n",
        "    return encoding\n",
        "\n",
        "def quantized_vector_to_base62(quantized_vector, n_bits):\n",
        "    # Convert quantized vector to a binary string with fixed width for each element\n",
        "    binary_str = ''.join([format(qv, f'0{n_bits}b') for qv in quantized_vector])\n",
        "    # Convert binary string to integer\n",
        "    num = int(binary_str, 2)\n",
        "    # Encode integer to base62\n",
        "    return base62_encode(num)\n",
        "\n",
        "# Example documents\n",
        "document1=\"\"\"\n",
        "Harvard Study Confirms Fluoride 'Significantly Lowers' Children's IQ\n",
        "\"\"\"\n",
        "\n",
        "document2=\"\"\"\n",
        "The government put fluoride in our water and attacked anyone who questioned it.\n",
        "\n",
        "Now - the NIH (after major pressure) has declared  it “reduces the IQ of children” and is “hazardous to human health” - and states are removing it from water.\n",
        "\n",
        "This is under-covered news.\n",
        "\"\"\"\n",
        "\n",
        "document3=\"\"\"\n",
        "Fluoride, often present in dental products and water supplies, is finally being recognized as a neurotoxin. Research indicates that excessive fluoride exposure, especially in children, is linked to reduced IQ scores and cognitive impairments. This neurotoxicity may stem from fluoride’s interference with neurotransmitter synthesis and its promotion of oxidative stress, raising concerns about its safety in vulnerable populations.\n",
        "\"Watch Attorney Michael Connett Depose the ‘Experts’ on the Safety of Fluoride in Drinking Water After New Data Shows Links to Lower IQ in Kids\n",
        "\"\"\"\n",
        "\n",
        "document4=\"\"\"\n",
        "“The proponents of this practice will often say there’s thousands of studies that show fluoridation is safe. Here I had the opportunity, under penalty of perjury, to ask these organizations … to point me to one study on this particular issue of the effects on the brain … Can you point me to any study that shows that water fluoridation is safe and every single one of the those organizations came back and said no.”#\n",
        "\"\"\"\n",
        "\n",
        "# Generate the semantic fingerprints\n",
        "fingerprint = generate_semantic_fingerprint(document1)\n",
        "fingerprint2 = generate_semantic_fingerprint(document2)\n",
        "\n",
        "# Apply 4-bit quantization to each fingerprint\n",
        "quantized_fingerprint_4bit = quantize_to_n_bits(fingerprint, 4)\n",
        "quantized_fingerprint2_4bit = quantize_to_n_bits(fingerprint2, 4)\n",
        "\n",
        "# Calculate cosine similarity for original and quantized vectors\n",
        "similarity_score = cosine_similarity([fingerprint], [fingerprint2])\n",
        "\n",
        "# Normalize quantized vectors to compute cosine similarity\n",
        "normalized_quantized_fingerprint_4bit = quantized_fingerprint_4bit / np.linalg.norm(quantized_fingerprint_4bit)\n",
        "normalized_quantized_fingerprint2_4bit = quantized_fingerprint2_4bit / np.linalg.norm(quantized_fingerprint2_4bit)\n",
        "similarity_score_4bit = cosine_similarity([normalized_quantized_fingerprint_4bit], [normalized_quantized_fingerprint2_4bit])\n",
        "\n",
        "print(\"Cosine Similarity Score (Original):\", similarity_score[0][0])\n",
        "print(\"Cosine Similarity Score (4-bit Quantized):\", similarity_score_4bit[0][0])\n",
        "\n",
        "# Encode quantized vectors to Base62 strings\n",
        "encoded_fingerprint = quantized_vector_to_base62(quantized_fingerprint_4bit, 4)\n",
        "encoded_fingerprint2 = quantized_vector_to_base62(quantized_fingerprint2_4bit, 4)\n",
        "\n",
        "print(\"Encoded Fingerprint:\", encoded_fingerprint)\n",
        "print(\"Encoded Fingerprint2:\", encoded_fingerprint2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import string\n",
        "\n",
        "# Initialize the SBERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def generate_semantic_fingerprint(document):\n",
        "    # Encode the document and return a 1D vector (embedding)\n",
        "    return model.encode([document])[0]\n",
        "\n",
        "def quantize_to_n_bits(vector, n_bits):\n",
        "    # Calculate the min and max of the vector for scaling\n",
        "    min_val, max_val = np.min(vector), np.max(vector)\n",
        "    # Calculate the number of bins for n-bit quantization (2^n)\n",
        "    n_bins = 2 ** n_bits\n",
        "    # Define bin edges\n",
        "    bins = np.linspace(min_val, max_val, n_bins)\n",
        "    # Quantize the vector to integer indices (bin assignments)\n",
        "    quantized_vector = np.digitize(vector, bins) - 1  # digitize starts from 1\n",
        "    # Clip values to ensure they're within [0, n_bins-1]\n",
        "    quantized_vector = np.clip(quantized_vector, 0, n_bins - 1)\n",
        "    return quantized_vector\n",
        "\n",
        "def base62_encode(num):\n",
        "    # Characters for base62 encoding\n",
        "    characters = string.digits + string.ascii_letters\n",
        "    base = 62\n",
        "    if num == 0:\n",
        "        return characters[0]\n",
        "    encoding = ''\n",
        "    while num > 0:\n",
        "        num, rem = divmod(num, base)\n",
        "        encoding = characters[rem] + encoding\n",
        "    return encoding\n",
        "\n",
        "\n",
        "def quantized_vector_to_base62(quantized_vector, n_bits):\n",
        "    # Convert quantized vector to a binary string with fixed width for each element\n",
        "    binary_str = ''.join([format(qv, f'0{n_bits}b') for qv in quantized_vector])\n",
        "    # Convert binary string to integer\n",
        "    num = int(binary_str, 2)\n",
        "    # Encode integer to base62\n",
        "    return base62_encode(num)\n",
        "\n",
        "def base62_decode(base62_str):\n",
        "    # Base62 文字セット\n",
        "    characters = string.digits + string.ascii_letters\n",
        "    base = 62\n",
        "    # Base62 を整数にデコード\n",
        "    num = 0\n",
        "    for char in base62_str:\n",
        "        num = num * base + characters.index(char)\n",
        "    return num\n",
        "\n",
        "def base62_to_quantized_vector(base62_str, n_bits, vector_length):\n",
        "    # Base62 文字列を整数にデコード\n",
        "    num = base62_decode(base62_str)\n",
        "    # 整数を2進数に変換し、ビット列に変換\n",
        "    binary_str = format(num, f'0{n_bits * vector_length}b')\n",
        "    # ビット列を n_bits ごとに分割して、元のベクトルを復元\n",
        "    quantized_vector = [int(binary_str[i:i + n_bits], 2) for i in range(0, len(binary_str), n_bits)]\n",
        "    return quantized_vector\n",
        "\n",
        "# Example documents\n",
        "documents = [\n",
        "    \"\"\"\n",
        "    Harvard Study Confirms Fluoride 'Significantly Lowers' Children's IQ\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    The government put fluoride in our water and attacked anyone who questioned it.\n",
        "    Now - the NIH (after major pressure) has declared  it “reduces the IQ of children” and is “hazardous to human health” - and states are removing it from water.\n",
        "    This is under-covered news.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Fluoride, often present in dental products and water supplies, is finally being recognized as a neurotoxin. Research indicates that excessive fluoride exposure, especially in children, is linked to reduced IQ scores and cognitive impairments. This neurotoxicity may stem from fluoride’s interference with neurotransmitter synthesis and its promotion of oxidative stress, raising concerns about its safety in vulnerable populations.\n",
        "    \"Watch Attorney Michael Connett Depose the ‘Experts’ on the Safety of Fluoride in Drinking Water After New Data Shows Links to Lower IQ in Kids\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Fluoride in tap water lowers the IQ of those who drink it\"\"\"\n",
        "]\n",
        "\n",
        "# Generate the semantic fingerprints and apply 4-bit quantization for each document\n",
        "fingerprints = [generate_semantic_fingerprint(doc) for doc in documents]\n",
        "quantized_fingerprints_4bit = [quantize_to_n_bits(fp, 4) for fp in fingerprints]\n",
        "\n",
        "# Normalize quantized vectors to compute cosine similarity\n",
        "normalized_quantized_fingerprints_4bit = [\n",
        "    qf / np.linalg.norm(qf) for qf in quantized_fingerprints_4bit\n",
        "]\n",
        "\n",
        "# Calculate and display cosine similarity for all combinations\n",
        "num_docs = len(documents)\n",
        "for i in range(num_docs):\n",
        "    for j in range(i + 1, num_docs):\n",
        "        similarity = cosine_similarity(\n",
        "            [normalized_quantized_fingerprints_4bit[i]],\n",
        "            [normalized_quantized_fingerprints_4bit[j]]\n",
        "        )[0][0]\n",
        "        print(f\"Cosine Similarity (4-bit Quantized) between Document {i+1} and Document {j+1}: {similarity}\")\n",
        "\n",
        "# Step 1: Generate embeddings for each document\n",
        "fingerprints = [generate_semantic_fingerprint(doc) for doc in documents]\n",
        "\n",
        "# Step 2: Calculate the average embedding (semantic centroid)\n",
        "average_embedding = np.mean(fingerprints, axis=0)\n",
        "\n",
        "quantized_average_fingerprints_4bit = quantize_to_n_bits(average_embedding, 4)\n",
        "normalized_quantized_average_fingerprints_4bit = quantized_average_fingerprints_4bit / np.linalg.norm(quantized_average_fingerprints_4bit)\n",
        "print(quantized_average_fingerprints_4bit)\n",
        "print(quantized_fingerprints_4bit[3])\n",
        "\n",
        "# Step 3: Calculate cosine similarity of each document embedding with the average embedding\n",
        "for idx, fingerprint in enumerate(fingerprints):\n",
        "    similarity = cosine_similarity([fingerprint], [average_embedding])[0][0]\n",
        "    print(f\"Cosine Similarity between Document {idx + 1} and Average Embedding: {similarity}\")\n",
        "\n",
        "print(cosine_similarity([normalized_quantized_average_fingerprints_4bit], [normalized_quantized_fingerprints_4bit[3]])[0][0])\n",
        "print(quantized_vector_to_base62(quantized_average_fingerprints_4bit,4))\n",
        "print(quantized_vector_to_base62(quantized_fingerprints_4bit[3],4))\n",
        "print(len(quantized_average_fingerprints_4bit))\n",
        "\n",
        "a = base62_to_quantized_vector('wjY7yyv2BKT4rLEfxgGFFeVDOQJhuImLTQOsqoA3yzFIuSa5au2s4b5OCt0nFp9C1kaJC79V1aazrvBPu1JFYwTNffHjqjzyyETjswiYwLCYDhISpKWaiLeV0UETv01tH213lzw8T57VA6YC1iCjl0ynFjIX7NK2pfgynMKhLteT3NLetA2ovnlSur7JRXPaKZItPJgkn0YQRYGq7fXsQzPaNIzhQRAubhbtirB021mVJ8mVsqe0VVmalVj9z7KasU',4,384)\n",
        "b = base62_to_quantized_vector('sFoVZtdk7HzSGC9YEpoxtGo9w1OThlXhW3Q4hg6sdrIIPB52kM2eTSrF0O8npcEKB2UFDHUVDv83YXgB64oWSblBM9NdtWd0n064zfPgXwWgBJkrGhJVTPQItIcZckXzccQwiKQS4qSMVyfJkgLGlJ9XGkUVOToXhelcuwh4AuPUJtqsqfztYPkgjYFwLIipvBzxJ9S0nxFA9Yin79qv52iHDq8KExVzsbGy5N2nkntD0ytvTRBrSzbKMlvJi9O5ok',4,384)\n",
        "a_norm = a / np.linalg.norm(a)\n",
        "b_norm = b / np.linalg.norm(b)\n",
        "print(cosine_similarity([a_norm],[b_norm]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNWSJ3hpaJur",
        "outputId": "b6956bcb-f8a3-45c5-bba2-008359286044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity (4-bit Quantized) between Document 1 and Document 2: 0.9533956402920651\n",
            "Cosine Similarity (4-bit Quantized) between Document 1 and Document 3: 0.9711414885395127\n",
            "Cosine Similarity (4-bit Quantized) between Document 1 and Document 4: 0.9641934862209532\n",
            "Cosine Similarity (4-bit Quantized) between Document 2 and Document 3: 0.9560595892495869\n",
            "Cosine Similarity (4-bit Quantized) between Document 2 and Document 4: 0.9543078528479654\n",
            "Cosine Similarity (4-bit Quantized) between Document 3 and Document 4: 0.9700528005847939\n",
            "[ 9  7  7  8 10  5 10 11  5  8  6  6  5  9  2  4  6  5  3  5  5  6 10  7\n",
            "  6 10  7  3  4  9 10  7  8  1  6  6  7  7  7  5  4  4 10  6  7  6  5  4\n",
            "  7  9  3  8  7 10  7  7  5  3  2 15  5  6  7  7 10 10  3  3  6  5  8  6\n",
            " 12  6  9  8  6  7 10  8  7  6 14  8 12  5  8  4  5  8  5  6  9  8  5  7\n",
            "  4  6  3  8  6  5  5  8  8  6  8  6  8  7  3 11  6  9  6  7  8  6  6  6\n",
            "  9  4  6  8  8  7  3  6  6  7 11  5  5  6  5  2  9  6  8  6  5  9  5  7\n",
            "  2  6  6  5  9  5  4  8  5  8  2  9  7  7  6  6  4  6  4  6  8  6  3 10\n",
            "  8  9  7  8 10  6  5  6  7  8  4  7  6  7  7  5  6  5  6  6  4  6  3  7\n",
            "  7  6  8  5  7  8  6  7  5  7  5  3  8  7  7  4 13  3  8  8  4  6  4  5\n",
            "  9  4 10  5  9  6  3  6  3  5  3  8  8  7 10  5 11  6  8  6  6  3  7  6\n",
            "  9  8  7  6  7  9  8  7  6 10  7  4  7  6  5 11  6  6  7  4  6  4  0  6\n",
            "  6  9 11  6  4 12  6  5  5  9  6  5  3  5  4  6  8  3  5 12  8  8  4  3\n",
            "  6  5  3 12 11  7  9  5 10  5  7  6  5 10  5  7  8  8  4  4  6  4  6  5\n",
            "  6  8  5  5  2  4  7  6 10  5  4  9  7  7  5  9  6  6  8 13  8  6  4  6\n",
            " 10  6  6  9  6  7  8  5  3  6  8  8  5  7  2  6  7  9  6  4  6  8  4  8\n",
            "  4  6  6  8  7  5  4  7 10  6  8  6  5  4  6 10  8  6  2  2 11  5  8  8]\n",
            "[10  6  6  6  9  3 11 11  5  6  6  5  5  6  3  2  5  5  3  4  5  2 10  6\n",
            "  6  8  7  4  5  9  9  9  8  0  4  6  6  7  7  4  4  5 11  5  6  7  4  6\n",
            "  7  8  1  9  6  9  7  8  5  3  3 15  6  4  7  7 11 10  1  5  5  4  7  6\n",
            " 13  6 11  7  5  7  8 10  7  6 13  9 11  5  8  2  5  7  4  7  8  6  4  7\n",
            "  6  6  3  9  4  4  6  8 11  6  6  7  8  6  3 11  7  9  5  8  8  5  5  5\n",
            "  8  2  6  8  8  8  3  6  5  5 11  2  5  4  5  1 11  6  8  6  5  8  5  5\n",
            "  1  6  4  5 11  4  3  7  4  6  4  9  6  7  6  5  4  6  5  5  7  5  2  8\n",
            "  8 10  9  7  9  5  5  5  6 10  3  7  7 10  8  2  6  4  6  6  0  5  3  8\n",
            "  9  8  8  3  9  8  6  7  4  8  5  2  8  7  8  4 13  4  7  9  5  6  5  3\n",
            " 11  3  9  5  9  7  3  6  4  4  4 10  9  6 12  4 11  6  9  7  5  5  7  7\n",
            " 11  8  7  8  6  9 11  6  4  9  8  3  6  4  5 11  5  4  7  3  7  3  0  8\n",
            "  6  9 11  3  3 12  7  3  4  9  6  4  4  6  5  4 11  2  3 12  9  9  5  2\n",
            "  6  5  3 13 11  6 10  3 11  6  7  5  3 10  6  5  7 10  3  4  5  5  6  4\n",
            "  6  9  6  4  3  4  9  6  8  6  3 10  6  7  4  9  5  5  9 13  8  7  4  5\n",
            "  9  6  5  9  5  5  8  5  4  6  8  7  7  6  2  7  7  9  7  6  7  5  3  8\n",
            "  3  6  5  7  7  4  4  8  9  5  8  7  6  3  6  9  7  6  3  1 11  5  6  6]\n",
            "Cosine Similarity between Document 1 and Average Embedding: 0.8837772607803345\n",
            "Cosine Similarity between Document 2 and Average Embedding: 0.8462854623794556\n",
            "Cosine Similarity between Document 3 and Average Embedding: 0.9107397198677063\n",
            "Cosine Similarity between Document 4 and Average Embedding: 0.9039649963378906\n",
            "0.9851083903982981\n",
            "wjY7yyv2BKT4rLEfxgGFFeVDOQJhuImLTQOsqoA3yzFIuSa5au2s4b5OCt0nFp9C1kaJC79V1aazrvBPu1JFYwTNffHjqjzyyETjswiYwLCYDhISpKWaiLeV0UETv01tH213lzw8T57VA6YC1iCjl0ynFjIX7NK2pfgynMKhLteT3NLetA2ovnlSur7JRXPaKZItPJgkn0YQRYGq7fXsQzPaNIzhQRAubhbtirB021mVJ8mVsqe0VVmalVj9z7KasU\n",
            "zvu9oFV1VXCFblV5rUcoA2z4mgNrZIVtAbwNbQ0w7fPTEJBED3rFcTkGUpMISepttzSNnjXUH36sbG1Mq6h4UzOAdn0Km9vg00LFhB7JFWLwOihanMqyb77JUuqWEAeIfWpToBi8mjRuuE77iErsFkbDptilbN5Nltcv1ZE0Ylt2jVXq3y3Nil1gGuMF0oDOUekzWWOXrVEGVE9a7fOZWbN4CSrxbs7trDl90cxhbzSJBJfCmQE1MHNIR8i114fr8y\n",
            "384\n",
            "[[0.99363846]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "prFupyw_FZ7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}